---
title: "re1 - digital soil mapping"
author: "Sujung Lee"
date: "2023-12-18"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE}
library(renv); library(here); library(tidyterra); library(visdat); library(ranger)
library(caret); library(Boruta); library(tibble); library(dplyr); library(pROC)
```

## 1. Data preparation
### 1.1. Load data
### 1.1.1. Soil samples
```{r}
# Load soil data from sampling locations
df_obs <- readr::read_csv(
  here::here("data-raw/soildata/berne_soil_sampling_locations.csv")
  )

# Display data
head(df_obs) |> 
  knitr::kable()
```

### 1.1.2. Environmental covariates
```{r}
# Get a list with the path to all raster files
list_raster <- list.files(
  here::here("data-raw/geodata/covariates/"),
  full.names = TRUE
  )

# Display data (lapply to clean names)
lapply(
  list_raster, 
  function(x) sub(".*/(.*)", "\\1", x)
  ) |> 
  unlist() |> 
  head(5) |> 
  print()
```

```{r}
# Load a raster file as example: Picking the slope profile at 2 m resolution
raster_example <- terra::rast(
  here::here("data-raw/geodata/covariates/Se_slope2m.tif")
  )
raster_example
```

## 1.2. Visualize data
```{r}
# To have some more flexibility, we can plot this in the ggplot-style as such:
ggplot2::ggplot() +
  tidyterra::geom_spatraster(data = raster_example) +
  ggplot2::scale_fill_viridis_c(
    na.value = NA,
    option = "magma",
    name = "Slope (%) \n"
    ) +
  ggplot2::theme_bw() +
  ggplot2::scale_x_continuous(expand = c(0, 0)) +  # avoid gap between plotting area and axis
  ggplot2::scale_y_continuous(expand = c(0, 0)) +
  ggplot2::labs(title = "Slope of the Study Area")
```

## 1.3. Combine data
```{r}
# Load all files as one batch
all_rasters <- terra::rast(list_raster)
all_rasters
```

```{r}
# Extract coordinates from sampling locations
sampling_xy <- df_obs |> 
  dplyr::select(x, y)

# From all rasters, extract values for sampling coordinates
df_covars <- terra::extract(
  all_rasters,  # The raster we want to extract from
  sampling_xy,  # A matrix of x and y values to extract for
  ID = FALSE    # To not add a default ID column to the output
  )

df_full <- cbind(df_obs, df_covars)
head(df_full) |> 
  knitr::kable() 
```

## 1.4. Data wrangling
```{r}
vars_categorical <- df_covars |> 
  
  # Get number of distinct values per variable
  dplyr::summarise(dplyr::across(dplyr::everything(), ~dplyr::n_distinct(.))) |> 
  
  # Turn df into long format for easy filtering
  tidyr::pivot_longer(
    dplyr::everything(), 
    names_to = "variable", 
    values_to = "n"
    ) |> 
  
  # Filter out variables with 10 or less distinct values
  dplyr::filter(n <= 10) |>
  
  # Extract the names of these variables
  dplyr::pull('variable')

cat("Variables with less than 10 distinct values:", 
    ifelse(length(vars_categorical) == 0, "none", vars_categorical))

df_full <- df_full |> 
  dplyr::mutate(
    dplyr::across(all_of(vars_categorical), ~as.factor(.)),
    dplyr::across(contains("waterlog"), ~as.factor(.)))
```


## 1.5. Checking missing data
```{r}
# Get number of rows to calculate percentages
n_rows <- nrow(df_full)

# Get number of distinct values per variable
df_full |> 
  dplyr::summarise(dplyr::across(dplyr::everything(), 
                                 ~ length(.) - sum(is.na(.)))) |> 
  tidyr::pivot_longer(dplyr::everything(), 
                      names_to = "variable", 
                      values_to = "n") |>
  dplyr::mutate(perc_available = round(n / n_rows * 100)) |> 
  dplyr::arrange(perc_available) |> 
  head(10) |> 
  knitr::kable()

df_full |> 
  dplyr::select(1:20) |>   # reduce data for readability of the plot
  visdat::vis_miss()
```

## 1.6. Save data
```{r}
if (!dir.exists(here::here("data"))) system(paste0("mkdir ", here::here("data")))
saveRDS(df_full, 
        here::here("data/df_full.rds"))
```


# 2. Train a random forest
## 2.1 Simple model
### 2.1.1 Load data
```{r}
df_full <- readRDS(here::here("data/df_full.rds"))

head(df_full) |> 
  knitr::kable()
```

### 2.1.2. Preparation
```{r}
# Specify target: The pH in the top 10cm
target <- "waterlog.100"

# Specify predictors_all: Remove soil sampling and observational data
predictors_all <- names(df_full)[14:ncol(df_full)]

cat("The target is:", target,
    "\nThe predictors_all are:", paste0(predictors_all[1:8], sep = ", "), "...")
```

```{r}
# Split dataset into training and testing sets
df_train <- df_full |> dplyr::filter(dataset == "calibration")
df_test  <- df_full |> dplyr::filter(dataset == "validation")

# Filter out any NA to avoid error when running a Random Forest
df_train <- df_train |> tidyr::drop_na()
df_test <- df_test   |> tidyr::drop_na()

# A little bit of verbose output:
n_tot <- nrow(df_train) + nrow(df_test)

perc_cal <- (nrow(df_train) / n_tot) |> round(2) * 100
perc_val <- (nrow(df_test)  / n_tot) |> round(2) * 100

cat("For model training, we have a calibration / validation split of: ",
    perc_cal, "/", perc_val, "%")
```

### 2.1.3 Model training
```{r}
# ranger() crashes when using tibbles, so we are using the
# base R notation to enter the data
rf_basic <- ranger::ranger( 
  y = df_train[, target],     # target variable
  x = df_train[, predictors_all], # Predictor variables
  seed = 42,                    # Specify the seed for randomization to reproduce the same model again
  num.threads = parallel::detectCores() - 1, # Use all but one CPU core for quick model training
  probability = TRUE) 

# Print a summary of fitted model
print(rf_basic)
```
To evaluate the model on the testing subset of the data, I used the confusionMatrix() function from the {caret} library.

```{r}
# Make classification predictions
x_pred <- predict(rf_basic, data = df_test, type = "response")$predictions
x <- as.factor(round(x_pred[,2])) # Using threshold as 0.5
Y <- df_test[, target] 

# Plot confusion matrix
conf_matrix <- caret::confusionMatrix(data = factor(x), reference = factor(Y))
print(conf_matrix)

```

###  Answer for 5.1 - Evaluation of the model on the testing subset of the data:

In evaluating the model on the testing subset, the overall accuracy stands at 0.76, indicating the proportion of correctly predicted instances. However, a closer look at the confusion matrix unveils an imbalance in the data, evident in the prevalence metric of 0.65. This imbalance highlights the frequency of the positive class (class 0), significantly influencing the interpretation of key metrics.

The elevated accuracy could be attributed to the model's proficiency in predicting the majority class. Yet, in imbalanced datasets, accuracy alone falls short of providing a comprehensive performance assessment. Crucial in such cases are sensitivity (True Positive Rate) and Positive Predictive Value (Precision).

Here, the model shows a sensitivity of 0.8, accurately identifying 80% of actual positive instances. The Positive Predictive Value, representing the accuracy of positive predictions, is calculated at 82.54%.

For a more holistic understanding, considering metrics that address class imbalances is essential. Both Balanced Accuracy and Kappa serve this purpose. The Balanced Accuracy, averaging sensitivity and specificity, is 0.7429, offering insights into the model's ability to generalize across both positive and negative instances while acknowledging the class imbalance.

Moreover, the Kappa statistic, registering at 0.4794, indicates a moderate level of agreement beyond what random chance would predict. The Kappa statistics show how well the model works overall, especially when imbalances could distort interpretations based solely on accuracy.


## 2.2 Variable selection
### 2.2.1 Variable importance
```{r}
# Let's run the basic model again but with recording the variable importance
rf_basic <- ranger::ranger( 
  y = df_train[, target],     # target variable
  x = df_train[, predictors_all],   # Predictor variables
  importance   = "permutation", # Pick permutation to calculate variable importance
  seed = 42,                    # Specify seed for randomization to reproduce the same model again
  num.threads = parallel::detectCores() - 1, # Use all but one CPU core for quick model training
  probability = TRUE) 

# Extract the variable importance and create a long tibble
vi_rf_basic <- rf_basic$variable.importance |>
  dplyr::bind_rows() |> 
  tidyr::pivot_longer(cols = dplyr::everything(), names_to = "variable")

# Plot variable importance, ordered by decreasing value
gg <- vi_rf_basic |> 
  ggplot2::ggplot(ggplot2::aes(x = reorder(variable, value), y = value)) +
  ggplot2::geom_bar(stat = "identity", fill = "grey50", width = 0.75) + 
  ggplot2::labs(
    y = "Change in OOB MSE after permutation", 
    x = "",
    title = "Variable importance based on OOB") +
  ggplot2::theme_classic() +
  ggplot2::coord_flip()

# Display plot
gg
```

### 2.2.2. Variable selection
```{r}
set.seed(42)

# run the algorithm
bor <- Boruta::Boruta(
    y = df_train[, target], 
    x = df_train[, predictors_all],
    maxRuns = 50, # Number of iterations. Set to 30 or lower if it takes too long
    num.threads = parallel::detectCores()-1)

# obtain results: a data frame with all variables, ordered by their importance
df_bor <- Boruta::attStats(bor) |> 
  tibble::rownames_to_column() |> 
  dplyr::arrange(dplyr::desc(meanImp))

# plot the importance result  
ggplot2::ggplot(ggplot2::aes(x = reorder(rowname, meanImp), 
                             y = meanImp,
                             fill = decision), 
                data = df_bor) +
  ggplot2::geom_bar(stat = "identity", width = 0.75) + 
  ggplot2::scale_fill_manual(values = c("grey30", "tomato", "grey70")) + 
  ggplot2::labs(
    y = "Variable importance", 
    x = "",
    title = "Variable importance based on Boruta") +
  ggplot2::theme_classic() +
  ggplot2::coord_flip()
```

```{r}
# get retained important variables
predictors_selected <- df_bor |> 
  dplyr::filter(decision == "Confirmed") |>
  dplyr::pull(rowname)

length(predictors_selected)
```

```{r}
# re-train Random Forest model
rf_bor <- ranger::ranger( 
  y = df_train[, target],              # target variable
  x = df_train[, predictors_selected], # Predictor variables
  seed = 42,                           # Specify the seed for randomization to reproduce the same model again
  num.threads = parallel::detectCores() - 1, # Use all but one CPU core for quick model training
  probability = TRUE) 

# quick report and performance of trained model object
rf_bor
```

### Evaluate model on the test set with rf_bor
```{r}
# Make predictions
x_pred_bor <- predict(rf_bor, data = df_test, type = "response")$predictions
x_bor <- as.factor(round(x_pred_bor[,2])) # Using threshold as 0.5
Y_bor <- df_test[, target] 

# Plot confusion matrix
conf_matrix_bor <- caret::confusionMatrix(data = factor(x_bor), reference = factor(Y_bor))
print(conf_matrix_bor)
```

### Answer for 5.2 - Comparison of two models
The model with the reduced set ('rf_bor') has a higher accuracy compared to the model with all covariates ('rf_basic') with an accuracy of 0.76. 
The Kappa statistic for the model with the reduced set is also higher compared to the model with all covariates, indicating a higher level of agreement beyond chance.
Sensitivity, specificity, and balanced accuracy are all higher for the model with the reduced set, suggesting better performance in correctly classifying both positive and negative instances.

### Answer for 5.2 - Consideration of OOB prediction error for the model choice
```{r}
oob_err_basic <- rf_basic$prediction.error
oob_err_bor <- rf_bor$prediction.error

cat("OOB prediction error from basic model:", oob_err_basic,
    ", OOB prediction error from Boruta model:", oob_err_bor)
```

A lower OOB error generally indicates better generalization to unseen data. In this comparison, the model with the reduced set of predictors (rf_bor) exhibits a lower OOB error compared to the model with all covariates (rf_basic). Therefore, considering both the performance metrics on the test set and the OOB error, the model with the reduced set informed by Boruta (rf_bor) is a preferred choice.

```{r}
# Save relevant data for model testing in the next chapter.
saveRDS(rf_bor,                   
        here::here("data/rf_for_waterlog-100.rds"))

saveRDS(df_train[, c(target, predictors_selected)],
        here::here("data/cal_for_waterlog-100.rds"))

saveRDS(df_test[, c(target, predictors_selected)],
        here::here("data/val_for_waterlog-100.rds"))
```

## 2.3 Model optimization
```{r}
# Set seed for reproducibility
set.seed(42)

# Read the reduced training and test data
df_train_sel <- readRDS(here::here("data/cal_for_waterlog-100.rds"))
df_test_sel <- readRDS(here::here("data/val_for_waterlog-100.rds"))

n_predictors <- length(predictors_selected)

pp <- recipes::recipe(waterlog.100 ~ ., data = df_train_sel)

rf_tuned <- caret::train(
  pp, 
  data = df_train_sel %>% 
    drop_na(), # Training data
  method = "ranger",             # Random Forest method
  trControl = trainControl(
    method = "cv", 
    number = 5, 
    savePredictions = "final"
    ),
  tuneGrid = expand.grid(
    .mtry = 1:floor(n_predictors / 3),
    .min.node.size = c(2, 5, 10, 20),
    .splitrule = "gini"
    ),
  
  metric = "Accuracy",
  replace = FALSE,
  sample.fraction = 0.5,
  num.trees = 2000,
  seed = 42
)

print(rf_tuned)
```

```{r}
# Train best model
mod_best <- caret::train(
  pp, 
  data = df_train_sel %>% 
    drop_na(), # Training data
  method = "ranger",             # Random Forest method
  trControl = trainControl(
    method = "cv", 
    number = 5, 
    savePredictions = "final"
    ),
  tuneGrid = expand.grid(
    .mtry = 6,
    .min.node.size = 10,
    .splitrule = "gini"
    ),
  
  metric = "Accuracy",
  replace = FALSE,
  sample.fraction = 0.5,
  num.trees = 2000,
  seed = 42
)


```

### Evaluate model on the test set with rf_bor
```{r}
# Make predictions
x_pred_bestmod <- predict(mod_best, newdata = df_test_sel)
Y_best <- df_test_sel[, target] 

# Plot confusion matrix
conf_matrix_best<- caret::confusionMatrix(data = factor(x_pred_bestmod), reference = factor(Y_best))
print(conf_matrix_best)
```

### Answer for 5.3
Through hyperparameter optimization with mtry set to 6 and min.node.size held constant at 10, the optimized model (mod_best) exhibits subtle but consistent improvements over the default model (rf_basic) when evaluated on the test set.

Mod_best demonstrates a slightly higher accuracy and enhanced sensitivity compared to rf_basic. In terms of specificity, mod_best also shows a marginal improvement with a higher value of specificity from rf_basic. This suggests comparable proficiency in correctly identifying negative instances, with mod_best showcasing a slight edge.

The Kappa statistic further supports the enhanced performance of mod_best compared to rf_basic's. This indicates that mod_best achieves a higher level of generalization to unseen data. Additionally, mod_best achieves a slightly higher balanced accuracy, reinforcing the marginal but consistent improvement in the optimized model.

In summary, based on these metrics, mod_best demonstrates a marginal but consistent enhancement over rf_basic in generalizing to unseen data.

## 2.4 Probabilistic predictions
```{r}
# re-train Random Forest model
rf_best <- ranger::ranger(
  y = df_train_sel[, target],              # target variable
  x = df_train_sel[, predictors_selected], # Predictor variables
  seed = 42,                           # Specify the seed for randomization to reproduce the same model again
  num.threads = parallel::detectCores() - 1, # Use all but one CPU core for quick model training
  mtry = 6,
  min.node.size = 10,  # or use the value from optimization
  probability = TRUE
)
```

```{r}
x_pred_best <- predict(rf_best, data = df_test_sel, type = "response")$predictions
x_best <- round(x_pred_best[,2]) # Using threshold as 0.5
Y_best <- df_test_sel[, target] 

# Create a ROC curve
roc_curve <- roc(Y_best, x_best)

# Plot the ROC curve
plot(roc_curve, main = "ROC Curve", col = "blue", lwd = 2)
```

### Answer for 5.4
In the given scenarios, the choice of the threshold for binary classification is intricately tied to the specific objectives and consequences associated with prediction errors. For the infrastructure construction project, prioritizing sensitivity is crucial, prompting the preference for a lower threshold. This choice aims to reduce the likelihood of false negatives, considering the severe threat posed by waterlogged soils to the stability of the building. The decision ensures that the model is more sensitive, classifying more areas as positive, even at the potential cost of introducing more false positives.

Conversely, a more balanced strategy is chosen for the project where waterlogged soils are unwanted but not critical. In this case, the threshold is adjusted to achieve a balanced trade-off between false positives and false negatives. This acknowledges that the two types of errors are not as serious as they would be in the infrastructure construction project.

