---
title: "re2 - spatial upscaling"
author: "Sujung Lee"
date: "2023-12-18"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE}
library(tidyverse); library(skimr); library(rsample); library(caret)
```

```{r}
df <- readr::read_csv("https://raw.githubusercontent.com/stineb/leafnp_data/main/data/leafnp_tian_et_al.csv")
```

```{r}
common_species <- df |> 
  group_by(Species) |> 
  summarise(count = n()) |> 
  arrange(desc(count)) |> 
  slice(1:50) |> 
  pull(Species)

dfs <- df |> 
  dplyr::select(leafN, lon, lat, elv, mat, map, ndep, mai, Species) |> 
  filter(Species %in% common_species)
  # group_by(lon, lat) |> 
  # summarise(across(where(is.numeric), mean))

# quick overview of data
skimr::skim(dfs)
```

```{r}
# show missing data
visdat::vis_miss(dfs)
```

# 1. Literature
## Answer for 1.1
Random cross-validation involves randomly partitioning the dataset into training and testing sets, neglecting the spatial arrangement of data points. This method gauges the model's performance within existing clusters but may overlook its capacity to generalize predictions to new spatial areas. However, Ploton et al. (2020) argued that random cross-validation may not be suitable when dealing with clustered reference data. In the presence of spatially clustered training data, random cross-validation primarily tests the model's ability to make predictions within these clusters.
In contrast, spatial cross-validation considers the spatial arrangement of data points when dividing the dataset. This approach evaluates the ability of the model not only within clustered training data but also in predicting outcomes in spatially distinct areas. It aims to address the limitations of random cross-validation by providing insights into the model's performance across diverse spatial contexts. However, it is crucial to acknowledge the caution raised by Wadoux et al. (2021) regarding spatial cross-validation, as they suggest that this method may result in overly pessimistic estimates of map accuracy.

## Answer for 1.2
Instead of relying solely on geographical distance, we can leverage the intrinsic connection between environmental covariates, such as climatic factors, and the spatial distribution of the target variable.
For instance, when considering climatic conditions like temperature, and precipitation, we can utilize the spatial patterns and correlations among these variables to inform our spatial upscaling. 
This approach enables the model to prioritize locations that not only share geographical proximity but also exhibit comparable climatic characteristics. As a result, prediction errors may be reduced, particularly in areas where the spatial distribution of the target variable is strongly influenced by climatic conditions. 

# 2. Random cross-validation
```{r}
# Data splitting
set.seed(123)  # for reproducibility
split <- rsample::initial_split(dfs, prop = 0.8)
df_train <- rsample::training(split)
df_test <- rsample::testing(split)

# The same model formulation is in the previous chapter
pp <- recipes::recipe(leafN ~ elv + mat + map + ndep + mai + Species, 
                      data = df_train) |> 
  recipes::step_center(recipes::all_numeric(), -recipes::all_outcomes()) |>
  recipes::step_scale(recipes::all_numeric(), -recipes::all_outcomes())

mod <- caret::train(
  pp, 
  data = df_train %>% 
    drop_na(), 
  method = "ranger",
  metric = "RMSE",
  trControl = trainControl(
    method = "cv",
    number = 5,
    savePredictions = "final"
    ),
  tuneGrid = expand.grid(
    .mtry = 3,        
    .min.node.size = 12,         
    .splitrule = "variance"      # default "variance"
  ),
  # arguments specific to "ranger" method
  replace = FALSE,
  sample.fraction = 0.5,
  #num.trees = 12,       
  seed = 123                     # for reproducibility
)

print(mod)
```
RMSE is 2.412978 and Rsquared is 0.7767409.

# 3. Spatial cross-validation
```{r}
library(ggplot2)
library(sf)
library(rnaturalearth)
library(rnaturalearthdata)

# get coast outline
coast <- rnaturalearth::ne_coastline(scale = 110, returnclass = "sf")

ggplot() +

  # plot coastline
  geom_sf(data = coast,
          colour = 'black',
          size = 0.2) +

  # set extent in longitude and latitude
  coord_sf(
    ylim = c(-60, 80),
    expand = FALSE) +  # to draw map strictly bounded by the specified extent
  
  # plot points on map
  geom_point(data = dfs, aes(x = lon, y = lat), color = "red", size = 0.2) +
  labs(x = "", y = "") +
  theme(legend.position = "bottom")
```

## Answer for 3.1
The spatial distribution of data points reveals a distinct clustering in Europe and East Asia, with a comparatively lower concentration in North America and South America. This pattern raises concerns about potential data bias or limitations in the data collection. If the data collection process exhibits a bias toward specific regions, the resulting outcomes may lack representativeness on a global scale. It is essential to critically assess and address any biases in data collection to ensure a more accurate understanding of the phenomenon's distribution across diverse geographical areas.

## Answer for 3.2
```{r}
# Cluster the data
clusters <- kmeans(dfs[,2:3], centers = 5)
dfs$clusters <- as.factor(clusters$cluster)

# Plot points
ggplot() +

  # plot coastline
  geom_sf(data = coast,
          colour = 'black',
          size = 0.2) +

  # set extent in longitude and latitude
  coord_sf(
    ylim = c(-60, 80),
    expand = FALSE) +  # to draw map strictly bounded by the specified extent
  
  # plot points on map
  geom_point(data = dfs, aes(x = lon, y = lat, color = clusters), size = 0.2) +
  scale_color_brewer(palette = "Set1") +  
  labs(x = "", y = "") +
  theme(legend.position = "bottom")
```


## Answer for 3.3
```{r}
# Cluster the data by leaf N
clusters_leafN <- kmeans(dfs[,1], centers = 5)
dfs$clusters_leafN <- as.factor(clusters_leafN$cluster)

# Plot the map
ggplot() +

  # plot coastline
  geom_sf(data = coast,
          colour = 'black',
          size = 0.2) +

  # set extent in longitude and latitude
  coord_sf(
    ylim = c(-60, 80),
    expand = FALSE) +  # to draw map strictly bounded by the specified extent
  
  # plot k-means clusters
  geom_point(data = dfs, aes(x = lon, y = lat, color = clusters_leafN), size = 0.2) +
  scale_color_brewer(palette = "Set1") +  
  labs(x = "", y = "") +
  theme(legend.position = "bottom")

```

## Answer for 3.4
```{r}
# create folds based on clusters
group_folds_train <- purrr::map(
  seq(length(unique(dfs$clusters))),
  ~ {
    dfs |> 
      select(clusters) |> 
      mutate(idx = 1:n()) |> 
      filter(clusters != .) |> 
      pull(idx)
  }
)

group_folds_test <- purrr::map(
  seq(length(unique(dfs$clusters))),
  ~ {
    dfs |> 
      select(clusters) |> 
      mutate(idx = 1:n()) |> 
      filter(clusters == .) |> 
      pull(idx)
  }
)

nam_target <- "leafN"
nams_predictors <- c("elv", "mat", "map", "ndep", "mai", "Species")

# create a function that trains a random forest model on a given set of rows and # predicts on a disjunct set of rows
train_test_by_fold <- function(dfs, idx_train, idx_val) {

  # training set
  df_train <- as.data.frame(dfs[idx_train, c(nam_target, nams_predictors)])
  
  # validation set
  df_valid <- as.data.frame(dfs[idx_val, c(nam_target, nams_predictors)])
  
  mod <- ranger::ranger(
    x = df_train[, nams_predictors],  # data frame with columns corresponding to predictors
    y = as.numeric(df_train[, nam_target])   # a vector of the target values (not a data frame!)
  )

  # Predict on the validation set
  pred <- predict(
    mod,       
    data = df_valid[, nams_predictors] 
  )$predictions

  rsq <- cor(pred, as.numeric(df_valid[, nam_target]))^2  # the R-squared determined on the validation set
  rmse <- sqrt(mean((pred - as.numeric(df_valid[, nam_target]))^2)) # the root mean square error on the validation set
  
  return(tibble(rsq = rsq, rmse = rmse))
}

# apply function on each custom fold and collect validation results in a nice
# data frame
out <- purrr::map2_dfr(
  group_folds_train,
  group_folds_test,
  ~train_test_by_fold(dfs, .x, .y)
) |> 
  mutate(test_fold = 1:5)

out
```

## Answer for 3.5
The random cross-validation yields a consolidated set of metrics summarizing the model's performance across the entire dataset. An RMSE of 2.412978 signifies the average difference between predicted and observed values, and the R-squared of 0.7767409 indicates the model's ability to explain approximately 77.7% of the variance in the response variable.

In contrast, spatial cross-validation provides a more nuanced perspective by considering the spatial structure of the data. The observed variability in rsq and rmse across different test folds implies that the model's performance varies spatially, aligning with distinct spatial clusters.

This discrepancy may stem from inadequately capturing spatial clusters within the data, where regions with unique spatial characteristics contribute to the observed variability in performance metrics. Additionally, the model's sensitivity to spatial structure plays a crucial role; its ability to generalize effectively in some regions may contrast with its challenges in others due to distinctive spatial characteristics.

# 4. Environmental cross-validation
## Answer for 4.1
```{r}
# Cluster by mat and map
clusters_env <- kmeans(dfs[,5:6], centers = 5)
dfs$clusters_env <- as.factor(clusters_env$cluster)

# create folds based on clusters
group_folds_train_env <- purrr::map(
  seq(length(unique(dfs$clusters_env))),
  ~ {
    dfs |> 
      select(clusters_env) |> 
      mutate(idx = 1:n()) |> 
      filter(clusters_env != .) |> 
      pull(idx)
  }
)

group_folds_test_env <- purrr::map(
  seq(length(unique(dfs$clusters_env))),
  ~ {
    dfs |> 
      select(clusters_env) |> 
      mutate(idx = 1:n()) |> 
      filter(clusters_env == .) |> 
      pull(idx)
  }
)

out_env <- purrr::map2_dfr(
  group_folds_train_env,
  group_folds_test_env,
  ~train_test_by_fold(dfs, .x, .y)
) |> 
  mutate(test_fold = 1:5)

out_env
```

## Answer for 4.2
The environmental cross-validation showed consistent improvement over the spatial cross-validation in terms of both Rsquared and RMSE. This suggests that the model might be more robust when considering environmental factors (here, the mean annual precipitation and the mean annual temperature). This result may stem from the increased relevance and informativeness of environmental attributes in predicting the target variable. Environmental factors, such as precipitation and temperature, could exhibit a more direct and influential relationship with the outcome, capturing essential patterns and variations in the data.
In contrast, the spatial cross-validation, which considers clusters of points in geographical space, may not fully account for the intricacies of environmental variations. Additionally, random cross-validation, while providing a general overview of model performance, might lack the specificity needed to capture the nuanced patterns associated with environmental conditions.
